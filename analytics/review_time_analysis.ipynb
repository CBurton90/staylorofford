{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse data\n",
    "def parseData(filename):\n",
    "\n",
    "    quake_cat = pd.read_csv(filename, parse_dates=[3, 4])\n",
    "\n",
    "    pd.set_option('display.max_columns', None)  # Allow all of the DataFrame to be printed\n",
    "\n",
    "    # Sort data by publicid, then by modification time\n",
    "\n",
    "    quake_cat = quake_cat.sort_values(by=['publicid','modificationtime'])\n",
    "\n",
    "    # Change negative longitudes to positive\n",
    "    \n",
    "    quake_cat.loc[quake_cat.longitude < 0, 'longitude'] += 360\n",
    "\n",
    "    return quake_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trim data by time\n",
    "def trimDataByTime(data, start_time, end_time):\n",
    "\n",
    "    data['time'] = pd.to_datetime(data['time']) # convert times to datetime64 objects\n",
    "    data = data.loc[((data['time'] >= pd.to_datetime(start_time)) &\n",
    "                               (data['time'] <= pd.to_datetime(end_time)))]\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns 3 dataframes: Manually reviewed, Manual reviewed with current magnitude, and Automatically reviewed.\n",
    "def returnDataFrames(data):\n",
    "\n",
    "    # Find the first manually reviewed origin for each eventid, if one exists\n",
    "\n",
    "    manual = data.loc[((data['evaluationmode'] == 'manual') | (data['evaluationstatus'] == 'preliminary')) & ((data['type'] == 'earthquake') | (data['type'] == 'outside of network interest'))]\n",
    "    manual = manual.sort_values(by=['modificationtime'])\n",
    "\n",
    "\n",
    "    # Find events which are not currently deleted.\n",
    "    exist = manual.groupby('publicid').nth(-1).reset_index()\n",
    "    exist = exist.loc[(exist['type'] == 'earthquake') | (exist['type'] == 'outside of network interest') | (exist['type'] == '')]\n",
    "\n",
    "\n",
    "    # Finding manual events continued.\n",
    "    manualMag = manual.groupby('publicid').nth(-1).reset_index()\n",
    "    manual = manual.groupby('publicid').nth(0).reset_index()\n",
    "\n",
    "\n",
    "    # Find those events with both a manual and automatic origin, then find the first automatic origin for such events\n",
    "\n",
    "    automatic = data.loc[(data['evaluationmode'] == 'automatic')]\n",
    "    automatic = automatic.loc[automatic['publicid'].isin(manual.publicid)]\n",
    "    automatic = automatic.sort_values(by=['modificationtime'])\n",
    "    automatic = automatic.groupby('publicid').nth(0).reset_index()\n",
    "\n",
    "\n",
    "    # Filter the first manual origins of events such that only those with a corresponding automatic origin are kept\n",
    "\n",
    "    manualMag = manualMag.loc[manualMag['publicid'].isin(automatic.publicid)]\n",
    "    manual = manual.loc[manual['publicid'].isin(automatic.publicid)]\n",
    "\n",
    "\n",
    "    # Filter out events that are currently deleted.\n",
    "\n",
    "    manualMag = manualMag.loc[manualMag['publicid'].isin(exist.publicid)]\n",
    "    manual = manual.loc[manual['publicid'].isin(exist.publicid)]\n",
    "    automatic = automatic.loc[automatic['publicid'].isin(exist.publicid)]\n",
    "\n",
    "\n",
    "    # Reset the index for the manual dataframe after matching against automatic events, then remove the old index\n",
    "    manual = manual.reset_index()\n",
    "    manual = manual.drop('index', 1)\n",
    "\n",
    "    # Reset the index for the manualMag dataframe after matching against automatic events, then remove the old index\n",
    "    manualMag = manualMag.reset_index()\n",
    "    manualMag = manualMag.drop('index', 1)\n",
    "\n",
    "    # Reset the index for the automatic dataframe after matching against exsiting events, then remove the old index\n",
    "    automatic = automatic.reset_index()\n",
    "    automatic = automatic.drop('index', 1)\n",
    "    \n",
    "    return manual,manualMag,automatic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now calculate the Review Time (RT) of an event as the difference between the first automatic origin ($AT^{1}$) and the first manual origin ($MT^{1}$). This assumes that the first automatic origin is coincident with the \"earthquake occurence in New Zealand\" as per the EQC contract.\n",
    "\n",
    "$RT = AT^{1} - MT^{1}$\n",
    "\n",
    "Add also to the DataFrame columns for analysis: magnitude bins, GHA shifts, and GHA colours for each event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns final dataframe to be used for analysis.\n",
    "def returnAnalysisDataFrame(manual, manualMag, automatic):\n",
    "\n",
    "    # Convert times to datetime objects\n",
    "\n",
    "    manual['modificationtime'] = pd.to_datetime(manual['modificationtime']).dt.tz_convert('UTC')\n",
    "    automatic['modificationtime'] = pd.to_datetime(automatic['modificationtime']).dt.tz_convert('UTC')\n",
    "\n",
    "    # Calculate review times\n",
    "\n",
    "    RT =  pd.DataFrame((manual['modificationtime'] - automatic['modificationtime']))\n",
    "    RT.columns = ['delta']\n",
    "\n",
    "    # Calculate review time in minutes for analysis\n",
    "\n",
    "    RT['minutes'] = (RT['delta'].dt.total_seconds()) / 60)\n",
    "\n",
    "    # Add other columns from the manual origin for use in analysis\n",
    "\n",
    "    RT['OT'] = manual['time']\n",
    "    RT['MT'] = manual['modificationtime']\n",
    "    RT['MT_NZT'] = manual['modificationtime'].dt.tz_convert('NZ')\n",
    "    RT['magnitude'] = manualMag['magnitude']\n",
    "    RT['latitude'] = manual['latitude']\n",
    "    RT['longitude'] = manual['longitude']\n",
    "    RT['publicid'] = manual['publicid']\n",
    "\n",
    "\n",
    "    # Add columns for use in data splitting: magnitude (rounded to nearest unit)\n",
    "\n",
    "    ### Floor magnitude to integer\n",
    "    RT['int_magnitude'] = RT['magnitude'].astype(int)\n",
    "\n",
    "    return RT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function for calculating a distribution\n",
    "def calculate_distribution(data_list, round_to):\n",
    "    \"\"\"\n",
    "    Calculate the distribution of a list of floats, where floats are rounded to the nearest round_to value.\n",
    "    :param data_list: list of data values\n",
    "    :param round_to: value to round data to for distribution calculation, e.g. 0.1 rounds data to the nearest 0.1\n",
    "    :return: data values in distribution for which data exists, number of data > 0 at each data value\n",
    "    \"\"\"\n",
    "    \n",
    "    # First, prepare data for distribution calculation (\"rounding\") and remove nan values (nn = non-nan)\n",
    "    \n",
    "    nn_rounded_data_list = []\n",
    "    for i in range(len(data_list)):\n",
    "        try:\n",
    "            if round_to <= 1:\n",
    "                # Here \"inflate\" decimal data to integers to make distribution calculation easier\n",
    "                nn_rounded_data_list.append(int(round((1 / round_to) * data_list[i])))\n",
    "            else:\n",
    "                # Here simply round the data to the nearest value\n",
    "                nn_rounded_data_list.append(int(round(data_list[i] / round_to)))\n",
    "        except:  # Fails on nan values\n",
    "            pass\n",
    "\n",
    "    # Prepare distribution lists\n",
    "        \n",
    "    data_counts = [0] * int(max(nn_rounded_data_list) + 1)\n",
    "    data_values = list(range(max(nn_rounded_data_list) + 1))\n",
    "\n",
    "    # Calculate distribution\n",
    "    \n",
    "    for i in data_values:\n",
    "        if nn_rounded_data_list.count(i) > 0:\n",
    "            data_counts[i] = nn_rounded_data_list.count(i)\n",
    "\n",
    "    # If rounding to a decimal, convert data values to decimal values\n",
    "            \n",
    "    if round_to <= 1:\n",
    "        for i in range(len(data_values)):\n",
    "            data_values[i] /= (1 / round_to)\n",
    "            \n",
    "    # For later analytical efficiency, remove points with 0 values in distribution\n",
    "\n",
    "    DVNZ = []  # data values non zero\n",
    "    DCNZ = []  # data counts non zero\n",
    "    for n in range(len(data_counts)):\n",
    "        if data_counts[n] > 0:\n",
    "            DVNZ.append(data_values[n])\n",
    "            DCNZ.append(data_counts[n])\n",
    "\n",
    "    return DVNZ, DCNZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all the review times for the events in our data, we need to study the data distribution to identify any outliers that could skew our statistics. To do this we use a distribution plot which sums rounded review times over the range of review times and plots the number of a given review time against the review time.\n",
    "\n",
    "By changing the rounding value we use in distribution calculation we can view data trends at different temporal scales. For example, rounding to the nearest 1440 will produce the review time distribution in nearest days, while rounding to 1 will produce the review time distribution in nearest minutes. Using a larger rounding value will show broader trends in time, but will reduce the fidelity of the data. It is good to use large rounding values when the distribution is not apparent, i.e. when data is scattered over time and does not cluster at low rounding values.\n",
    "\n",
    "Because the data are contaminated by many aspects of the earthquake location system and operational process, there are many filtering options available below. I recommend the following strategy:\n",
    "\n",
    "1. Disable all filtering by using None or False parameters where appropriate and run the distribution plotting.\n",
    "1. Investigate the unfiltered distribution at multiple rounding values to see trends over different time scales, then determine if you want to add time filtering.\n",
    "1. Run the distribution with time filtering, adjusting it as appropriate until it contains only the desired data.\n",
    "1. Now, if you like, begin playing with time windowing to look at distribution trends over time.\n",
    "1. Similarly, if you like, try the different data splitting options to investigate how different aspects of the data contribute to the overall distribution. You can only split the data one way at a time using these options, but you can focus the splitting on certain subsets of the data by setting the other splitting parameters to given values in the relevant columns of the DataFrame, e.g. to see the distribution of review times split by magnitude for the green colour on the early shift have:\n",
    "```\n",
    "by_magnitude = True\n",
    "by_shift = 'early'\n",
    "by_colour = 'green'\n",
    "```\n",
    "\n",
    "There are few limits on how much you can spread the data out in this plotting, so be careful not to overwhelm yourself!\n",
    "\n",
    "Note: all time filtering is done using the **origin time** of events, so if a review occurs in a subsequent time window (high review time) it will be associated with events in a preceding window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data distribution(s)\n",
    "def plot(RT, filter_window, filter_box, start_time, end_time, by_magnitude, rounding_value,\n",
    "         time_window, window_overlap, doPlot, doPrint):\n",
    "\n",
    "    # Set the colours to use in plotting from the shift sequence colours (assuming max subsets for plotting = 7!)\n",
    "    colours = ['black','red','green','purple','yellow','blue','orange']\n",
    "\n",
    "    # Apply temporal filtering\n",
    "\n",
    "    if filter_window:\n",
    "        print('Filtering review times to those between limits: [' + str(filter_window[0]) + ' - ' + \n",
    "              str(filter_window[1]) + '] minutes')\n",
    "        print('Number of values before filtering is: ' + str(len(RT['minutes'].values)))\n",
    "        if filter_window[0] and filter_window[1]:\n",
    "            analysis_data = RT.loc[(RT['minutes'] >= filter_window[0]) & (RT['minutes'] <= filter_window[1])]\n",
    "        elif filter_window[0] and not filter_window[1]:\n",
    "            analysis_data = RT.loc[(RT['minutes'] >= filter_window[0])]\n",
    "        elif filter_window[1] and not filter_window[0]:\n",
    "            analysis_data = RT.loc[(RT['minutes'] <= filter_window[1])]\n",
    "        elif not filter_window[0] and not filter_window[1]:\n",
    "            analysis_data = RT\n",
    "        #print('Number of values remaining after filtering is: ' + str(len(analysis_data['minutes'].values)))\n",
    "    else:\n",
    "        print('No data filtering has been performed.')\n",
    "        analysis_data = RT\n",
    "\n",
    "\n",
    "    # Apply spatial filtering\n",
    "\n",
    "    if filter_box:\n",
    "        print('Filtering earthquakes to those in the box between latitudes '\n",
    "             + str(filter_box[0]) + ' and ' + str(filter_box[1]) + ' and longitudes ' + str(filter_box[2])\n",
    "             + ' and ' + str(filter_box[3]))\n",
    "        analysis_data = analysis_data.loc[((analysis_data['latitude'] >= filter_box[0]) &\n",
    "                                         (analysis_data['latitude'] <= filter_box[1]) &\n",
    "                                         (analysis_data['longitude'] >= filter_box[2]) &\n",
    "                                         (analysis_data['longitude'] <= filter_box[3]))]\n",
    "    else:\n",
    "        print('No spatial filtering has been performed.')\n",
    "        print('Number of values remaining after boundary filtering is: ' + str(len(analysis_data['minutes'].values)))\n",
    "\n",
    "\n",
    "    # Set up splitting factors\n",
    "\n",
    "    if by_magnitude == True:\n",
    "        print('Splitting data by magnitude...')\n",
    "        splitting_column = 'int_magnitude'\n",
    "        splitting_factors = ['all', 1, 2, 3, 4, 5, 6]\n",
    "    else:\n",
    "        print('No data splitting will be performed.')\n",
    "        splitting_factors = ['all']\n",
    "\n",
    "    # Apply any factor fixing to the data\n",
    "\n",
    "    if by_magnitude and by_magnitude != True:\n",
    "        print('Subsetting data to that with integer magnitude == ' + str(by_magnitude) + ' ...')\n",
    "        analysis_data = analysis_data.loc[(analysis_data['int_magnitude'] == by_magnitude)]\n",
    "\n",
    "    # Initiate time windowing of data\n",
    "\n",
    "    if time_window:\n",
    "        print('Binning data by time windows...')\n",
    "        window_start = min(analysis_data['OT'].values)\n",
    "        window_end = window_start + np.timedelta64(time_window, 's')\n",
    "    else:\n",
    "        print('No data time binning has been performed.')\n",
    "        time_window = 1  # This will break the while loop following the first while loop iteration\n",
    "        window_overlap = 0\n",
    "        window_start = min(analysis_data['OT'].values)\n",
    "        window_end = max(analysis_data['OT'].values)\n",
    "\n",
    "    # Run the data analysis to generate the distribution(s)\n",
    "\n",
    "    print('Calculating distribution(s)...')\n",
    "\n",
    "    analytics_data_types = ['data_values', 'data_counts', 'median', '95_perc', 'start_time', 'end_time', 'split_idx']\n",
    "    analytics_data = [[] for i in range(len(analytics_data_types))]\n",
    "\n",
    "    last_data = max(analysis_data['OT'].values)\n",
    "    \n",
    "    while window_end <= last_data:\n",
    "\n",
    "        # Gather data for current time window\n",
    "\n",
    "        current_window_data = analysis_data.loc[(analysis_data['OT'].values >= window_start) & \n",
    "                                               (analysis_data['OT'].values <= window_end)]        \n",
    "\n",
    "        # Apply data splitting\n",
    "\n",
    "        for i in range(len(splitting_factors)):\n",
    "\n",
    "            # Gather data for current split\n",
    "\n",
    "            if splitting_factors[i] == 'all':  # Analyse all data in the time window first\n",
    "                current_data = current_window_data\n",
    "            else:\n",
    "                current_data = current_window_data.loc[current_window_data[splitting_column] == splitting_factors[i]]\n",
    "\n",
    "            # Calculate the data distribution\n",
    "\n",
    "            data_list = list(current_data['minutes'].values)\n",
    "            if len(data_list) > 0:\n",
    "                data_values, data_counts = calculate_distribution(data_list, rounding_value)\n",
    "\n",
    "                # Store the data for analysis\n",
    "\n",
    "                analytics_data[0].append(data_values)\n",
    "                analytics_data[1].append(data_counts)\n",
    "                analytics_data[2].append((round(np.percentile(data_list, 50), 2)))\n",
    "                analytics_data[3].append((round(np.percentile(data_list, 95), 2)))\n",
    "                analytics_data[4].append(str(window_start)[:10])\n",
    "                analytics_data[5].append(str(window_end)[:10])\n",
    "                analytics_data[6].append(i)\n",
    "\n",
    "        # Update time loop parameters\n",
    "\n",
    "        window_start += np.timedelta64(round(time_window * (1 - window_overlap)), 's')  \n",
    "        window_end += np.timedelta64(round(time_window * (1 - window_overlap)), 's')\n",
    "\n",
    "    print('Start Time: ' + start_time)\n",
    "    print('End Time: ' + end_time)\n",
    "    print('Boundary: MinLat: '\n",
    "             + str(filter_box[0]) + ' MaxLat: ' + str(filter_box[1]) + ' MinLon: ' + str(filter_box[2])\n",
    "             + ' MaxLon: ' + str(filter_box[3]))\n",
    "\n",
    "    if doPlot == True:\n",
    "        \n",
    "        # Plot the distributions\n",
    "        \n",
    "        print('Plotting distribution(s)...')\n",
    "        print('-----------------------------------------------------------------------')\n",
    "\n",
    "        # Plot the distribution(s)\n",
    "\n",
    "        if len(analytics_data[0]) > 5:\n",
    "            y_len = (12 * (len(analytics_data[0]) / 6))\n",
    "        else:\n",
    "            y_len = 12\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(12, y_len))\n",
    "\n",
    "        badges = []\n",
    "        max_data_counts = []\n",
    "        y_labels = []\n",
    "        for i in range(len(analytics_data[0])):\n",
    "\n",
    "            # Prepare y labels\n",
    "\n",
    "            max_data_count = max(analytics_data[1][i])\n",
    "            y_labels.append('start = ' + analytics_data[4][i] + \n",
    "                            '\\nend = ' + analytics_data[5][i] + \n",
    "                            '\\nmax = ' + str(max_data_count) + \n",
    "                            '\\nN = ' + str(sum(analytics_data[1][i])) + \n",
    "                            '\\nmedian = ' + str(analytics_data[2][i]) + \n",
    "                            '\\n95% = ' + str(analytics_data[3][i]))\n",
    "\n",
    "            # Normalise each distribution\n",
    "\n",
    "            normalised_data_counts = []\n",
    "            for j in range(len(analytics_data[0][i])):\n",
    "                normalised_data_counts.append((analytics_data[1][i][j] / max_data_count) + i - 0.5)\n",
    "\n",
    "            # Plot the medians and 95% values of each distribution\n",
    "\n",
    "            plt.plot([analytics_data[2][i]] * 2, [min(normalised_data_counts), max(normalised_data_counts) - 0.1],\n",
    "                     color='k', linestyle='--', linewidth=1)\n",
    "            plt.plot([analytics_data[3][i]] * 2, [min(normalised_data_counts), max(normalised_data_counts) - 0.1],\n",
    "                     color='k', linestyle='-.', linewidth=1)\n",
    "\n",
    "            # Plot each distribution\n",
    "\n",
    "            plt.plot(analytics_data[0][i], normalised_data_counts, color=colours[analytics_data[6][i]], linewidth=2)\n",
    "\n",
    "            badges.append(patches.Patch(color=colours[analytics_data[6][i]]))\n",
    "\n",
    "        # Add figure formatting\n",
    "\n",
    "        ax.set_yticks(list(range(len(analytics_data[0]))))\n",
    "        ax.set_yticklabels(y_labels)\n",
    "        plt.locator_params(axis='x', nbins=10)\n",
    "        plt.xlabel('review time value (scale 1:' + str(rounding_value) + ' minutes)', labelpad=15, fontsize=14)\n",
    "\n",
    "        badges = badges[:len(splitting_factors)]\n",
    "        badges.reverse()\n",
    "        plt.legend(handles=badges, fontsize=14, framealpha=1, borderpad=1, labelspacing=0.75, handlelength=2,\n",
    "                  columnspacing=1)\n",
    "\n",
    "        plt.subplots_adjust(left=0.13, right=0.98, bottom=0.12, top=0.9)\n",
    "        fig.suptitle('review time distribution with data rounded to nearest ' + str(rounding_value) + \n",
    "                     ' minutes', y=0.93, fontsize=14)\n",
    "\n",
    "        # Show the plot!\n",
    "\n",
    "        plt.savefig('distribution.png', format='png', dpi=300)\n",
    "        plt.show()\n",
    "        \n",
    "    if doPrint == True:\n",
    "        \n",
    "        # Return the review time data in a csv-ready format\n",
    "        \n",
    "        print('\\nstart time of time window, time window length, number of events, median event review time')\n",
    "        for i in range(len(analytics_data[0])):\n",
    "            print(str(analytics_data[4][i]) + ',' + str(time_window) + ',' + str(sum(analytics_data[1][i])) + ',' + str(analytics_data[2][i]))      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set Parameters\n",
    "\n",
    "#File\n",
    "csv_filename = './quakehistory-13102019.csv'\n",
    "\n",
    "#Time window\n",
    "start_time = '2018-12-12T00:00:00+00'\n",
    "end_time = '2019-11-01T00:00:00+00'\n",
    "\n",
    "time_range = [start_time, end_time]\n",
    "\n",
    "# Spatial data filtering\n",
    "\n",
    "boundaries = [-90, 90, -180, 180]  # minlat, maxlat, minlon, maxlon for box to analyse earthquake review times in\n",
    "\n",
    "# Set parameters for distribution plotting: time window for rolling analysis, \n",
    "# data splitting/fixing (by magnitude), and RT value rounding.\n",
    "# Note that RT filtering time windowing, and data splitting are all optional, but rounding is required.\n",
    "\n",
    "# Data filtering by RT value\n",
    "\n",
    "filter_window = None  # data subset for distribution plotting (in minutes),\n",
    "                               # set to None to perform no data subsetting,\n",
    "                               # set either limit to None to exclude filtering by this limit\n",
    "\n",
    "# Data splitting/fixing by magnitude\n",
    "\n",
    "by_magnitude = False  # whether to split data by magnitude, or which magnitude data to include \"fix\" in plotting\n",
    "\n",
    "# What value to round RT values to\n",
    "        \n",
    "rounding_value = 1  # number to round to in minutes, can be any real number\n",
    "\n",
    "# Split data by time windows around the data's modification time (needs resetting after each iteration)\n",
    "        \n",
    "time_window = 86400      # time window to bin data by in seconds, set to None to perform no windowing\n",
    "window_overlap = 0   # percentage each time window should overlap (disabled by time_window = None,\n",
    "                     # otherwise must be float)\n",
    "\n",
    "# Whether to plot or print review time data, set by the appropriate boolean\n",
    "\n",
    "doPlot = False\n",
    "doPrint = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Execute code\n",
    "\n",
    "parsedData = parseData(csv_filename)\n",
    "\n",
    "data = trimDataByTime(parsedData, time_range[0], time_range[1])\n",
    "\n",
    "manual, manualMag, automatic = returnDataFrames(data)\n",
    "\n",
    "df = returnAnalysisDataFrame(manual, manualMag, automatic)\n",
    "\n",
    "plot(df, filter_window, boundaries, time_range[0], time_range[1], by_magnitude, \n",
    "     rounding_value, time_window, window_overlap, doPlot, doPrint)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
